{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for CUDA availablilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check the installation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3kyrCyR4qU8-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3u3tzpEqU1t",
    "outputId": "8e72dfef-894a-411b-b63b-6ad3d7afc481"
   },
   "outputs": [],
   "source": [
    "# Load ResNet50 pre-trained model without the top layer (include_top=False)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of ResNet50\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers for classification\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification (fake vs real)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Dataset Imbalaceing for agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hTOUqaEGaco",
    "outputId": "8584729a-aade-4976-e39e-3592408e83cc"
   },
   "outputs": [],
   "source": [
    "#checking if data is balanced or unbalanced\n",
    "\n",
    "# Count images in each class for train, validation, and test sets\n",
    "import numpy as np\n",
    "\n",
    "train_counts = np.unique(train_data.classes, return_counts=True)\n",
    "val_counts = np.unique(val_data.classes, return_counts=True)\n",
    "test_counts = np.unique(test_data.classes, return_counts=True)\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Training set: {dict(zip(train_data.class_indices.keys(), train_counts[1]))}\")\n",
    "print(f\"Validation set: {dict(zip(val_data.class_indices.keys(), val_counts[1]))}\")\n",
    "print(f\"Test set: {dict(zip(test_data.class_indices.keys(), test_counts[1]))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Traing (Epoch = 10, Batch= 32, Img_size= 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "D2QLiTDqqUzW",
    "outputId": "d24dff38-a3f0-46a7-dc09-2cc86a675135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112002 images belonging to 2 classes.\n",
      "Found 7885 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alank\\anaconda3\\envs\\deepfake_env\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.7605 - loss: 0.5568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alank\\anaconda3\\envs\\deepfake_env\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m936s\u001b[0m 266ms/step - accuracy: 0.7605 - loss: 0.5568 - val_accuracy: 0.8205 - val_loss: 0.3937\n",
      "Epoch 2/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1101s\u001b[0m 314ms/step - accuracy: 0.8005 - loss: 0.4178 - val_accuracy: 0.8339 - val_loss: 0.3553\n",
      "Epoch 3/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m985s\u001b[0m 281ms/step - accuracy: 0.8109 - loss: 0.4024 - val_accuracy: 0.8377 - val_loss: 0.3561\n",
      "Epoch 4/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1025s\u001b[0m 293ms/step - accuracy: 0.8151 - loss: 0.3933 - val_accuracy: 0.8375 - val_loss: 0.3494\n",
      "Epoch 5/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1376s\u001b[0m 393ms/step - accuracy: 0.8186 - loss: 0.3876 - val_accuracy: 0.8412 - val_loss: 0.3491\n",
      "Epoch 6/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1379s\u001b[0m 394ms/step - accuracy: 0.8229 - loss: 0.3787 - val_accuracy: 0.8417 - val_loss: 0.3457\n",
      "Epoch 7/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1436s\u001b[0m 410ms/step - accuracy: 0.8248 - loss: 0.3755 - val_accuracy: 0.8436 - val_loss: 0.3449\n",
      "Epoch 8/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 270ms/step - accuracy: 0.8251 - loss: 0.3730 - val_accuracy: 0.8413 - val_loss: 0.3452\n",
      "Epoch 9/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m938s\u001b[0m 268ms/step - accuracy: 0.8266 - loss: 0.3692 - val_accuracy: 0.8491 - val_loss: 0.3357\n",
      "Epoch 10/10\n",
      "\u001b[1m3501/3501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m911s\u001b[0m 260ms/step - accuracy: 0.8284 - loss: 0.3664 - val_accuracy: 0.8383 - val_loss: 0.3464\n",
      "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 209ms/step - accuracy: 0.8472 - loss: 0.3466\n",
      "Test Accuracy: 84.83%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = 128  # Reduced image size for faster training\n",
    "BATCH_SIZE = 32  # Increased batch size for fewer steps per epoch\n",
    "EPOCHS = 10\n",
    "\n",
    "# Data Generators with Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255, validation_split=0.2,\n",
    "    horizontal_flip=True, rotation_range=10)\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    \"Dataset/Train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    \"Dataset/Validation\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Model Definition using a lightweight MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint('best_deepfake_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate(val_data)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfromance Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0z1JxVNlqe-x",
    "outputId": "2d9a8654-94d8-4d4d-addf-9b538d350439"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)  # Read the image\n",
    "    img = cv2.resize(img, (128, 128))  # Resize the image to match the model input size\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize the image\n",
    "    img_array = img.reshape((1, 128, 128, 3))  # Add batch dimension\n",
    "    return img, img_array\n",
    "\n",
    "# Specify the model path\n",
    "model_path = r'C:\\Users\\alank\\OneDrive\\Desktop\\deepfake detection\\best_deepfake_model.keras'\n",
    "\n",
    "# Load the model\n",
    "best_model = load_model(model_path)\n",
    "\n",
    "# Provide the image path to test\n",
    "img_path = 'C:\\Users\\alank\\OneDrive\\Desktop\\deepfake detection\\Dataset\\Test\\Real\\real_1.jpg'  # Replace with your image path\n",
    "\n",
    "# Load and preprocess the image\n",
    "img, img_array = load_and_preprocess_image(img_path)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = best_model.predict(img_array)[0][0]\n",
    "\n",
    "# Classify the result based on the prediction value (threshold 0.5)\n",
    "result = 'Real' if prediction >= 0.5 else 'Fake'\n",
    "confidence = prediction if result == 'Real' else 1 - prediction  # Confidence score for either class\n",
    "\n",
    "# Print the result and confidence\n",
    "print(f\"Prediction: {result}\")\n",
    "print(f\"Confidence Score: {confidence:.4f}\")\n",
    "\n",
    "# Display the image with result and confidence score\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert color to RGB for display\n",
    "plt.title(f\"{result} - {confidence*100:.2f}% confidence\")\n",
    "plt.axis('off')  # Hide axes for better image display\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deepfake_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
